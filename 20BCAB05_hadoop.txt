Terminal 1:

#to find current directory
[cloudera@quickstart ~]$ pwd
/home/cloudera

#copying the file from local fs to HDFS
[cloudera@quickstart ~]$ hadoop fs -copyFromLocal /home/cloudera/Desktop/dept.csv saizel/dept.csv
[cloudera@quickstart ~]$ hadoop fs -copyFromLocal /home/cloudera/Desktop/emp.csv saizel/emp.csv
[cloudera@quickstart ~]$ hadoop fs -ls saizel



HIVE

[cloudera@quickstart ~]$ hive
hive> show databases;

#creating database
hive> create database newdb;

#for internal/managed table
hive> create table if not exists emp(eid int, ename string, epos string, esal int, ecom int, edpno int) row format delimited fields terminated by ',';
hive> describe emp;

#for external table
hive> create external table if not exists empext(eid int, ename string, epos string, esal int, ecom int, edpno int) row format delimited fields terminated by '/user/cloudera/saizel/empext';

#for loading values of the table into emp table, we created
hive> load data local inpath '/home/cloudera/Desktop/emp.csv' into table emp;

#for selecting all rows
hive> select * from emp;




PIG

[cloudera@quickstart ~]$ hadoop fs -mkdir saizel1
[cloudera@quickstart ~]$ hadoop fs -copyFromLocal /home/cloudera/Desktop/emp.csv saizel1/emp.csv

#launching pig
[cloudera@quickstart ~]$ pig

#changing directory to saizel1
grunt> cd saizel1

#loading emp file to variable A
grunt> A = load 'emp.csv' using PigStorage(',') as (eid: int, ename: chararray, epos:chararray, esal:int,ecom:int, edpno: int);

#gives top 5 rows
grunt> D = limit A 5;
grunt> dump D;

#sorting the values in A
grunt> E = order A by esal;
grunt> dump E;
grunt> F = order A by esal desc;
grunt> dump F;

#using Store
grunt> store F into '/user/cloudera/saizel1/pigout/' using PigStorage(',');

#generating the values by eid
grunt> G = foreach A generate eid;
grunt> dump G;

#generating the values and new columns bonus and incentive
grunt> H = foreach A generate * , ecom * 3 as bonus, esal * 5 as incentive;
grunt> dump H;

#generating the values where only first three letters will be displayed
grunt> I = foreach A generate SUBSTRING(ename,0,3);
grunt> dump I;

#generating the values for 1 and 2nd columns
grunt> J = foreach A generate $0,$1;
grunt> dump J;

#grouping the values by edpno
grunt> K = group A  by edpno;  
grunt> dump K;

#grouping the values by edpno and epos
grunt> M = group A by (edpno, epos);
grunt> dump M;

grunt> SPLIT A into B if edpno==10, C if edpno==20, D if epos=='Manager';

JOINS

(in another terminal)

[cloudera@quickstart ~]$ hadoop fs -copyFromLocal /home/cloudera/Desktop/dept.csv saizel1/dept.csv

(pig)

grunt> A = load 'emp.csv' using PigStorage(',') as (eid: int, ename: chararray, epos:chararray, esal:int,ecom:int, edpno: int);
grunt> B = load 'dept.csv' using PigStorage(',') as (edpno: int, epos:chararray,ecity: chararray);

#using full join
grunt> C = JOIN A by edpno, B by edpno;
grunt> dump C;
grunt> D = foreach C generate A::eid,B::epos;
grunt> dump D;
grunt> E = JOIN A by edpno RIGHT OUTER, B by edpno;
grunt> E = JOIN A by edpno LEFT OUTER, B by edpno;

WORD COUNT
(in another terminal)

[cloudera@quickstart ~]$ hadoop fs -mkdir saizel2
[cloudera@quickstart ~]$ hadoop fs -copyFromLocal /home/cloudera/Desktop/v.txt saizel2/v.txt

(PIG)

#reading the txt file we created
grunt> cat saizel2/v.txt
hi
hi
i

#obtaining lines from the text file
grunt> lines = load '/user/cloudera/saizel2/v.txt' as (line:chararray);
grunt> dump lines;

#obtaining tokens from the lines
grunt> token = foreach lines generate TOKENIZE(line);
grunt> dump token;

#obtaining values in 1st column
grunt> flats = foreach token generate FLATTEN($0);   
grunt> dump flats;

#obtaining values grouped by same words
grunt> group_words = group flats by $0;
grunt> dump group_words;

#obtaining the counted words 
grunt> count_words = foreach group_words generate group as word, COUNT($1) as word_occurence;
grunt> dump count_words;

















